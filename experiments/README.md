# BLUFF Benchmark Experiments

This directory contains the complete experimental framework for the BLUFF (Benchmark for Linguistic Understanding of Fake/real text and Fabrication) benchmark, evaluating **Veracity Classification** (disinformation detection) and **Human-Machine Text Detection** (Turing test) across 79 languages.

---

## Table of Contents

1. [Overview](#overview)
2. [Tasks](#tasks)
3. [Training Settings](#training-settings)
4. [Data Sources](#data-sources)
5. [Language Classifications](#language-classifications)
6. [Models](#models)
7. [Directory Structure](#directory-structure)
8. [Data Preparation](#data-preparation)
9. [Running Experiments](#running-experiments)
10. [Evaluation Metrics](#evaluation-metrics)
11. [Zero-shot Prompts](#zero-shot-prompts)
12. [Configuration Details](#configuration-details)
13. [Troubleshooting](#troubleshooting)

---

## Overview

The BLUFF benchmark evaluates cross-lingual transfer and multilingual performance for:
- **Disinformation detection** across diverse languages
- **AI-generated text detection** (Turing test)

### Key Features
- **79 languages** spanning 14 language families
- **4 tasks** (2 binary, 2 multi-class)
- **6 training settings** (4 cross-lingual + 1 multilingual + 1 external evaluation)
- **18 models** (11 encoders + 7 decoders)
- **Stratified sampling** by language and class

---

## Tasks

### Task 1: Veracity Binary Classification
**Classes**: Real (genuine news) vs Fake (disinformation)

| Label | Description |
|-------|-------------|
| Real | Genuine, factual news content |
| Fake | Disinformation, misinformation, or manipulated content |

### Task 2: Veracity Multi-class Classification (8 classes)
Combines veracity with authorship type:

| Class | Veracity | Authorship | Description |
|-------|----------|------------|-------------|
| HumanReal | Real | HWT | Genuine news written by human |
| MachineReal | Real | MGT | Genuine news generated by AI |
| HumanAIReal | Real | HAT | Genuine news from human-AI collaboration |
| TranslatedReal | Real | MTT | Genuine news that was machine-translated |
| HumanFake | Fake | HWT | Disinformation written by human |
| MachineFake | Fake | MGT | Disinformation generated by AI |
| HumanAIFake | Fake | HAT | Disinformation from human-AI collaboration |
| TranslatedFake | Fake | MTT | Disinformation that was machine-translated |

### Task 3: Authorship Binary Classification
**Classes**: Human vs Machine

| Label | Authorship Types Included |
|-------|---------------------------|
| Human | HWT (Human-Written Text) only |
| Machine | MGT (Machine-Generated Text) only |

### Task 4: Authorship Multi-class Classification (4 classes)

| Class | Description |
|-------|-------------|
| HWT | Human-Written Text - Original content by humans |
| MGT | Machine-Generated Text - AI-generated content |
| MTT | Machine-Translated Text - Human content translated by machine |
| HAT | Human-AI collaborative Text - Content edited/augmented by human-AI collaboration |

---

## Training Settings

### 1. Cross-lingual: Big-head â†’ Long-tail
- **Train**: All 20 big-head (high-resource) languages
- **Validation**: 10% of big-head data
- **Test Big-head**: 10% of big-head data
- **Test Long-tail**: 100% of long-tail languages (zero-shot transfer)

### 2. Cross-lingual: By Language Family
- **Train**: One language family at a time
- **Test**: All other language families
- Measures within-family vs cross-family transfer

### 3. Cross-lingual: By Script Type
- **Train**: One script type at a time
- **Test**: All other script types
- Evaluates script-based transfer patterns

### 4. Cross-lingual: By Syntactic Typology
- **Train**: One word order type at a time
- **Test**: All other word orders
- Assesses syntax-based transfer

### 5. Multilingual: Joint Training
- **Train**: 80% of ALL languages (stratified by language + class)
- **Validation**: 10% of all languages
- **Test Big-head**: 10% of big-head portion
- **Test Long-tail**: 10% of long-tail portion

### 6. External Evaluation (Encoder-only)
- **Train**: 90% of BLUFF data only (BLUFF AI + BLUFF Human)
- **Validation**: 10% of BLUFF data
- **Test**: 100% of Disinfo All dataset (completely external data)
- **Purpose**: Evaluate model generalization to unseen external data sources
- **Note**: This setting is only for encoder models (fine-tuning). Decoder models use zero-shot inference and don't require this separate evaluation.

---

## Data Sources

### BLUFF AI (AI-Generated Translations)
**Location**: `organized_translations/ai_generated/`

| Text Field | Language Column | Authorship Logic |
|------------|-----------------|------------------|
| `article_content` | `language(article_content)` | MGT if degree âˆˆ {critical, complete}, HAT if degree âˆˆ {light, minor, medium, moderate} |
| `translated_content` | `language(translated_content)` | MTT (always) |
| `post_content` | `language(post_content)` | MGT if degree âˆˆ {critical, complete}, HAT if degree âˆˆ {light, minor, medium, moderate} |
| `translated_post` | `language(translated_post)` | MTT (always) |

**Veracity**: From `veracity` column (`fake_news` â†’ Fake, `real_news` â†’ Real)

### BLUFF Human (Human-Written Fact-checks)
**Location**: `organized_translations/human_written/`

| Text Field | Authorship |
|------------|------------|
| `article_content` | HWT |
| `translated_article` | MTT |
| `post_content` | MGT |
| `translated_post` | MTT |

**Veracity**: From `veracity` column (normalized to Real/Fake)

### Disinfo All (Consolidated Disinformation)
**Location**: `organized_translations/all_disinfo_data/GREENLAND_Consolidated_Disinfromation.csv`

| Text Field | Authorship |
|------------|------------|
| `text` | HWT |

**Veracity**: From `label` column (`True` â†’ Real, `False` â†’ Fake)

---

## Language Classifications

### Big-head Languages (20)
High-resource languages with substantial web content:

```python
BIG_HEAD = [
    'eng',  # English
    'deu',  # German
    'nld',  # Dutch
    'spa',  # Spanish
    'por',  # Portuguese
    'fra',  # French
    'ita',  # Italian
    'pol',  # Polish
    'rus',  # Russian
    'ces',  # Czech
    'ukr',  # Ukrainian
    'fas',  # Persian
    'ell',  # Greek
    'ara',  # Arabic
    'tur',  # Turkish
    'jpn',  # Japanese
    'kor',  # Korean
    'ind',  # Indonesian
    'vie',  # Vietnamese
    'zho',  # Chinese
]
```

### Long-tail Languages (59)
Low-resource languages (all languages not in big-head list).

### Language Families

| Family | Languages |
|--------|-----------|
| Indo-European | eng, deu, nld, spa, por, fra, ita, pol, rus, ces, ukr, fas, ell, afr, dan, nor, swe, ron, cat, hrv, bos, bul, srp, slk, slv, mkd, hin, ben, pan, mar, guj, nep, urd, sin, kur, sqi, lit, lav, asm, ori, glg |
| Sino-Tibetan | zho, mya |
| Tai-Kadai | tha |
| Afro-Asiatic | ara, heb, som, hau, orm, amh, ful |
| Turkic | tur, aze, kaz |
| Japonic | jpn |
| Koreanic | kor |
| Austronesian | ind, msa, tgl, ban |
| Austroasiatic | vie |
| Niger-Congo | swa, ibo, zul |
| Dravidian | tam, tel, mal |
| Uralic | hun, fin, est |
| Kartvelian | kat |
| Tupian | grn |
| Creole | hat, jam, pap |
| Constructed | epo |

### Script Types

| Script | Languages |
|--------|-----------|
| Latin | eng, spa, por, deu, fra, ita, nld, pol, ces, vie, tur, ind, hrv, ron, bos, sqi, afr, cat, srp, slk, slv, dan, nor, swe, lav, lit, grn, hat, jam, pap, est, fin, hun, msa, tgl, swa, som, hau, ibo, zul, aze, ban, glg, epo, orm, ful |
| Cyrillic | rus, ukr, bul, srp, mkd, kaz |
| Arabic | ara, urd, kur, fas |
| CJK | zho, kor, jpn |
| Greek | ell |
| Hebrew | heb |
| Indic (Devanagari/Brahmic) | hin, mar, nep, ben, tam, tel, mal, guj, sin, pan, asm, ori |
| Georgian | kat |
| Myanmar | mya |
| Thai | tha |
| Ethiopic (Ge'ez) | amh |

### Syntactic Typology (Word Order)

| Word Order | Languages |
|------------|-----------|
| SVO | eng, spa, por, fra, ita, nld, pol, rus, ces, ukr, vie, zho, ind, ell, hrv, ron, bos, bul, sqi, srp, slk, slv, mkd, msa, sin, cat, lav, lit, tgl, afr, dan, nor, swe, heb, tha, hat, jam, pap, swa, ibo, zul, glg, epo, hau, ful |
| SOV | tur, jpn, fas, kor, hin, tam, tel, mal, aze, ben, mya, mar, guj, nep, urd, kur, kat, grn, pan, amh, asm, ori, kaz, som, orm |
| VSO | ara |
| Free | deu, hun, fin, ban, est |

---

## Models

### Encoder Models (Fine-tuned)

| ID | Model Name | HuggingFace ID | Parameters | Languages |
|----|------------|----------------|------------|-----------|
| mBERT | Multilingual BERT | `bert-base-multilingual-cased` | 177M | 104 |
| mDeBERTa | mDeBERTa-v3 | `microsoft/mdeberta-v3-base` | 278M | 100 |
| XLM-R | XLM-RoBERTa XXL | `facebook/xlm-roberta-xxl` | 10.7B | 100 |
| XLM-100 | XLM-MLM-100 | `FacebookAI/xlm-mlm-100-1280` | 570M | 100 |
| XLM-17 | XLM-MLM-17 | `FacebookAI/xlm-mlm-17-1280` | 570M | 17 |
| XLM-B | Bernice | `jhu-clsp/bernice` | 550M | 100 |
| XLM-T | Twitter-XLM-RoBERTa | `cardiffnlp/twitter-xlm-roberta-base` | 278M | 30 |
| XLM-E | InfoXLM | `microsoft/infoxlm-large` | 559M | 100 |
| XLM-V | XLM-V | `microsoft/xlm-v-base` | 560M | 100 |
| S-BERT | LaBSE | `sentence-transformers/LaBSE` | 470M | 109 |

### Decoder Models (Zero-shot Inference)

| Model Name | HuggingFace ID | Parameters | Notes |
|------------|----------------|------------|-------|
| Mistral-7B | `mistralai/Mistral-7B-Instruct-v0.3` | 7B | Uses chat template |
| Llama-3.1-8B | `meta-llama/Llama-3.1-8B-Instruct` | 8B | Requires transformersâ‰¥4.43.0 |
| Llama-3.2-1B | `meta-llama/Llama-3.2-1B-Instruct` | 1B | Lightweight, 128k context |
| Qwen3-8B | `Qwen/Qwen3-8B` | 8B | Requires transformersâ‰¥4.51.0, uses sampling (temp=0.7) |
| Qwen3-4B | `Qwen/Qwen3-4B` | 4B | Requires transformersâ‰¥4.51.0, uses sampling (temp=0.7) |
| Llama-4-Scout | `meta-llama/Llama-4-Scout-17B-16E-Instruct` | 17B (MoE) | Multimodal, uses AutoProcessor |
| GPT-OSS-20B | `openai/gpt-oss-20b` | 21B (3.6B active) | Uses harmony format, MoE |

---

## Directory Structure

```
experiments/
â”œâ”€â”€ README.md                              # This file
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ languages.py                       # Language classifications
â”‚   â”œâ”€â”€ models.py                          # Model configurations
â”‚   â””â”€â”€ paths.py                           # Data and output paths
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ master_dataset.parquet         # Combined dataset with all metadata
â”‚   â”‚   â””â”€â”€ stats.json                     # Dataset statistics
â”‚   â”œâ”€â”€ splits/
â”‚   â”‚   â”œâ”€â”€ cross_lingual_bighead_longtail/
â”‚   â”‚   â”‚   â”œâ”€â”€ train.json
â”‚   â”‚   â”‚   â”œâ”€â”€ val.json
â”‚   â”‚   â”‚   â”œâ”€â”€ test_bighead.json
â”‚   â”‚   â”‚   â””â”€â”€ test_longtail.json
â”‚   â”‚   â”œâ”€â”€ cross_lingual_family/
â”‚   â”‚   â”‚   â””â”€â”€ {family_name}/
â”‚   â”‚   â”‚       â”œâ”€â”€ train.json
â”‚   â”‚   â”‚       â”œâ”€â”€ val.json
â”‚   â”‚   â”‚       â””â”€â”€ test.json
â”‚   â”‚   â”œâ”€â”€ cross_lingual_script/
â”‚   â”‚   â”‚   â””â”€â”€ {script_name}/
â”‚   â”‚   â”‚       â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ cross_lingual_syntax/
â”‚   â”‚   â”‚   â””â”€â”€ {word_order}/
â”‚   â”‚   â”‚       â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ multilingual/
â”‚   â”‚   â”‚   â”œâ”€â”€ train.json
â”‚   â”‚   â”‚   â”œâ”€â”€ val.json
â”‚   â”‚   â”‚   â”œâ”€â”€ test_bighead.json
â”‚   â”‚   â”‚   â””â”€â”€ test_longtail.json
â”‚   â”‚   â””â”€â”€ external_evaluation/           # Train on BLUFF, test on Disinfo All
â”‚   â”‚       â”œâ”€â”€ train.json                 # BLUFF AI + BLUFF Human only
â”‚   â”‚       â”œâ”€â”€ val.json
â”‚   â”‚       â”œâ”€â”€ test.json                  # Disinfo All only
â”‚   â”‚       â”œâ”€â”€ test_bighead.json
â”‚   â”‚       â””â”€â”€ test_longtail.json
â”‚   â””â”€â”€ prepare_data.py                    # Data preparation script
â”œâ”€â”€ task1_veracity_binary/
â”‚   â”œâ”€â”€ cross_lingual_bighead_longtail/
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â”œâ”€â”€ evaluate.py
â”‚   â”‚   â””â”€â”€ results/
â”‚   â”œâ”€â”€ cross_lingual_family/
â”‚   â”œâ”€â”€ cross_lingual_script/
â”‚   â”œâ”€â”€ cross_lingual_syntax/
â”‚   â””â”€â”€ multilingual/
â”œâ”€â”€ task2_veracity_multiclass/
â”‚   â””â”€â”€ ... (same structure)
â”œâ”€â”€ task3_authorship_binary/
â”‚   â””â”€â”€ ... (same structure)
â”œâ”€â”€ task4_authorship_multiclass/
â”‚   â””â”€â”€ ... (same structure)
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ encoders.py                        # Encoder model wrappers
â”‚   â”œâ”€â”€ decoders.py                        # Decoder inference utilities
â”‚   â””â”€â”€ trainer.py                         # Training loop utilities
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ metrics.py                         # Metric calculations
â”‚   â””â”€â”€ analysis.py                        # Per-language/family analysis
â””â”€â”€ scripts/
    â”œâ”€â”€ run_all_experiments.sh             # Master runner script
    â”œâ”€â”€ run_task1_veracity_binary.sh
    â”œâ”€â”€ run_task2_veracity_multiclass.sh
    â”œâ”€â”€ run_task3_authorship_binary.sh
    â””â”€â”€ run_task4_authorship_multiclass.sh
```

---

## Data Preparation

### Step 1: Generate Master Dataset

```bash
cd experiments
python data/prepare_data.py
```

This creates `data/processed/master_dataset.parquet` with columns:

| Column | Description |
|--------|-------------|
| `uuid` | Unique identifier |
| `text` | The text content |
| `text_type` | "article" or "post" |
| `language` | ISO 639-3 language code |
| `veracity` | "real" or "fake" |
| `authorship` | "HWT", "MGT", "MTT", or "HAT" |
| `degree` | Editing degree (for HAT samples) |
| `source_dataset` | "bluff_ai", "bluff_human", or "disinfo_all" |
| `original_text_field` | Original column name |
| `language_family` | Genetic language family |
| `script_type` | Writing script |
| `word_order` | Syntactic word order |
| `is_bighead` | True if big-head language |

### Step 2: Generate Splits

```bash
python data/prepare_data.py --generate-splits
```

This creates train/val/test splits for all 6 training settings.

### Sampling Strategy
- **Max 1000 samples per language** for training
- **Proportional mix** of articles and posts (based on availability)
- **Stratified** by language and class
- **Class balancing** using Disinfo All dataset for True samples

---

## Running Experiments

### Prerequisites

```bash
pip install torch transformers datasets pandas pyarrow scikit-learn tqdm accelerate
```

### ðŸ“Š Experiment Status and Management

#### Check Current Experiment Status
```bash
# Show comprehensive status of all experiments (completed vs missing)
python create_experiment_status_table.py
```

This will show you:
- Total experiments: 208 (88 encoder + 120 decoder experiments)
- Completion status by model type and task
- Detailed list of missing experiments organized by priority

#### Analyze Existing Decoder Results
```bash
# Analyze all existing decoder results and create tables
python analyze_decoder_results.py
```

This script:
- Searches for all decoder result files automatically
- Extracts F1 scores and creates the requested table format
- Shows results for all three prompt regimes: crosslingual, native, translate
- Saves results to `decoder_results_analysis.csv`

### ðŸ¤– Running Decoder Experiments

#### Comprehensive Decoder Experiments (All 3 Prompt Regimes)
```bash
# Run all missing decoder experiments
python run_decoder_experiments.py

# Run specific models (priority order: small â†’ medium â†’ large)
python run_decoder_experiments.py --models Gemma-3-270M Qwen3-0.6B Gemma-3-1B Llama-3.2-1B

# Run specific prompt modes only
python run_decoder_experiments.py --modes native translate

# Run specific tasks only
python run_decoder_experiments.py --tasks veracity_binary authorship_binary

# Quick test with limited samples
python run_decoder_experiments.py --max-test-samples 1000

# Disable 4-bit quantization (requires more memory)
python run_decoder_experiments.py --no-4bit
```

#### Single Decoder Model Test
```bash
# Test individual model on one task
python run_single_experiment.py --model Mistral-7B --task veracity_binary --decoder --decoder-mode crosslingual
```

### ðŸ§  Running Encoder Experiments

#### Single Encoder Experiment
```bash
# Run individual encoder experiment
python run_single_experiment.py --model mBERT --task veracity_binary --setting cross_lingual_bighead_longtail

# Example: Multilingual training
python run_single_experiment.py --model XLM-R --task authorship_binary --setting multilingual

# With custom hyperparameters
python run_single_experiment.py --model mDeBERTa --task veracity_multiclass --setting multilingual --epochs 5 --batch-size 8 --learning-rate 1e-5
```

#### Legacy Methods (For Reference)

```bash
# Run all experiments (legacy)
cd experiments/scripts
bash run_all_experiments.sh

# Run individual task (legacy)
bash run_task1_veracity_binary.sh
bash run_task2_veracity_multiclass.sh
bash run_task3_authorship_binary.sh
bash run_task4_authorship_multiclass.sh
```

### ðŸŽ¯ Decoder Prompt Regimes Explained

The decoder experiments use three different prompt regimes:

1. **Crosslingual** (`--decoder-mode crosslingual`)
   - English prompts + native language text
   - Default mode for zero-shot inference

2. **Native** (`--decoder-mode native`)
   - Native language prompts + native language text
   - Uses translated prompts in the text's language

3. **English-Translate** (`--decoder-mode translate`)
   - English prompts + English translated text
   - All text is first translated to English

---

## Evaluation Metrics

### Primary Metric
- **Macro-F1**: Averaged across all classes (handles class imbalance)

### Additional Metrics
- **Accuracy**: Overall correct predictions
- **Precision**: Per-class and macro-averaged
- **Recall**: Per-class and macro-averaged
- **AU-ROC**: Area under ROC curve (binary tasks)

### Breakdown Dimensions
Results are reported for:
- Overall performance
- Big-head vs Long-tail languages
- Per language
- Per language family
- Per script type
- Per word order

### Output Format

Each experiment produces:

```
results/
â”œâ”€â”€ config.json              # Hyperparameters and settings
â”œâ”€â”€ metrics_overall.json     # Overall metrics
â”œâ”€â”€ metrics_by_language.json # Per-language breakdown
â”œâ”€â”€ metrics_by_family.json   # Per-family breakdown
â”œâ”€â”€ metrics_by_script.json   # Per-script breakdown
â”œâ”€â”€ metrics_by_syntax.json   # Per-word-order breakdown
â”œâ”€â”€ predictions.parquet      # Raw predictions
â”œâ”€â”€ confusion_matrix.png     # Confusion matrix visualization
â””â”€â”€ training_log.json        # Training curves (for encoders)
```

---

## Zero-shot Prompts

### Veracity Binary

```
Classify the following text as either 'Real' (genuine news) or 'Fake' (disinformation).
Only respond with a single word: Real or Fake

Text: {text}

Label:
```

### Veracity Multi-class (8 classes)

```
Classify the following text into exactly one category:
- HumanReal (genuine news written by human)
- MachineReal (genuine news generated by AI)
- HumanFake (disinformation written by human)
- MachineFake (disinformation generated by AI)
- HumanAIReal (genuine news edited by human-AI collaboration)
- TranslatedReal (genuine news that was machine-translated)
- HumanAIFake (disinformation edited by human-AI collaboration)
- TranslatedFake (disinformation that was machine-translated)

Only respond with a single word from the list above.

Text: {text}

Label:
```

### Authorship Binary

```
Determine if the following text was written by a Human or generated by a Machine.
Only respond with a single word: Human or Machine

Text: {text}

Label:
```

### Authorship Multi-class (4 classes)

```
Classify the authorship of the following text into exactly one category:
- HWT (Human-Written Text)
- MGT (Machine-Generated Text)
- MTT (Machine-Translated Text)
- HAT (Human-AI collaborative Text)

Only respond with the three-letter code: HWT, MGT, MTT, or HAT

Text: {text}

Label:
```

### Decoder Settings
- **Max tokens**: 10
- **Temperature**: 0 (deterministic)
- **Text truncation**: None (use full text)
- **Invalid outputs**: Stored in `raw_outputs` column for later salvage

---

## Configuration Details

### Hyperparameters (Encoders)

| Parameter | Value |
|-----------|-------|
| Learning rate | 2e-5 |
| Batch size | 16 (adjust for GPU memory) |
| Epochs | 3 |
| Weight decay | 0.01 |
| Warmup ratio | 0.1 |
| Max sequence length | 512 (model-dependent) |
| Optimizer | AdamW |
| Scheduler | Linear with warmup |

### Data Sampling

| Parameter | Value |
|-----------|-------|
| Max samples per language | 1000 |
| Train/Val/Test split | 80/10/10 |
| Article/Post ratio | Proportional to availability |
| Random seed | 42 |

---

## Troubleshooting

### Out of Memory (OOM)

1. Reduce batch size: `--batch_size 8` or `--batch_size 4`
2. Enable gradient checkpointing: `--gradient_checkpointing`
3. Use mixed precision: `--fp16`
4. For XLM-RoBERTa-XXL, use `--model xlm-roberta-base` instead

### Missing Languages

Some models don't support all 79 languages. The framework will:
1. Log a warning for unsupported languages
2. Skip those samples during training
3. Report "N/A" for those languages in results

### Invalid Decoder Outputs

Invalid outputs are stored in `predictions.parquet` with:
- `predicted_label`: "unknown"
- `raw_output`: The actual model output
- Use `evaluation/salvage_outputs.py` to attempt recovery

### Slow Training

1. Ensure GPU is being used: Check `torch.cuda.is_available()`
2. Use DataLoader workers: `--num_workers 4`
3. Pre-tokenize dataset: `python data/prepare_data.py --tokenize`

---

## Citation

If you use this benchmark, please cite:

```bibtex
@inproceedings{lucas2026bluff,
    title     = {{BLUFF}: A Benchmark for Linguistic Understanding of Fake-news Forensics},
    author    = {Lucas, Jason and Lee, Dongwon},
    booktitle = {Proceedings of the 32nd ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)},
    year      = {2026},
    publisher = {ACM},
    address   = {Toronto, Canada}
}
```

---

## Contact

For questions or issues, please open an issue on the repository.

---

*Last updated: February 2026*
